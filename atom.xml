<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Simon - all articles</title>
    <link href="http://notsimon.github.io/atom.xml" rel="self" />
    <link href="http://notsimon.github.io" />
    <id>http://notsimon.github.io/atom.xml</id>
    <author>
        <name>Simon Guillot</name>
        <email>sgr.[last name]@gmail.com</email>
    </author>
    <updated>2017-05-10T00:00:00Z</updated>
    <entry>
    <title>Real time interpolation of the magnetic field</title>
    <link href="http://notsimon.github.io/articles/2017-05-10-magnetic-field-interpolation.html" />
    <id>http://notsimon.github.io/articles/2017-05-10-magnetic-field-interpolation.html</id>
    <published>2017-05-10T00:00:00Z</published>
    <updated>2017-05-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<style>
  .profile {
    display: none;
  }

  article {
    margin-top: 10vh;
  }
</style>

<article>
  <header>
    <h1>Real time interpolation of the magnetic field</h1>
    
    <p><time>May 10, 2017</time></p>
    
  </header>

  <section class="content">
    <p>This report introduces one of my attempts at modelling the magnetic field of a room using a continuous representation. The first goal of this work is to assist the tracking of the orientation of a device by taking into account the multiple distortions of the field observed indoor: this is a step toward a positioning algorithm that takes advantage of these anomalies.</p>
<h2 id="maxwells-equations-and-the-magnetic-scalar-potential">Maxwell’s equations and the magnetic scalar potential</h2>
<p>In order to be plausible, an approximation of the magnetic field should obey some properties well known by physicists. All electromagnetic phenomena are governed by <a href="https://en.wikipedia.org/wiki/Maxwell%27s_equations#Formulation_in_SI_units_convention">Maxwell’s equations</a>, two of them involve the magnetic field:</p>
<p><span class="math display">\displaystyle 
  \nabla \cdot B = 0
</span></p>
<p><span class="math display">\displaystyle 
  \nabla \times B = \mu_0 \left(J + \epsilon_0 \frac{\partial E}{\partial t}\right)
</span></p>
<p>where <span class="math inline">B \in \mathbb{R}^3 \mapsto \mathbb{R}^3</span> is the magnetic field, <span class="math inline">\mu_0</span> and <span class="math inline">\epsilon_0</span> are respectively the permeability and permittivity of vacuum, <span class="math inline">J</span> is the current density and <span class="math inline">E</span> the electric field.</p>
<p>The first of these two laws states that the magnetic field is divergence free, in other words, <em>magnetic monopoles do not exist</em>. We are assuming that there is no free current in the air and no time-dependent effects due to moving electric charges, thus the Ampère’s circuital law reduces to:</p>
<p><span class="math display">\displaystyle 
  \nabla \times B = 0
</span></p>
<p>In these conditions, the magnetic field is then said to be <em>irrotational</em>. An irrotational vector field can be fully described as the gradient of a scalar field, called a scalar potential:</p>
<p><span class="math display">\displaystyle 
  B = - \nabla \psi
</span></p>
<p>Thus, by modeling <span class="math inline">\psi</span> in place of <span class="math inline">B</span> directly, we are implicitly modeling a vector field that follows the second law.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> The first law will be added as a regularization in the optimization procedure.</p>
<h2 id="differentiable-interpolation">Differentiable interpolation</h2>
<p>The map is represented by a set of anchor points <span class="math inline">M = \{(c_k, w_k) | k \in [1, K]\}</span>, where <span class="math inline">c_k</span> is the position of the anchor in <span class="math inline">\mathbb{R}^2</span> and <span class="math inline">w_k</span> its associated value – a scalar potential in this case.</p>
<p>In order to have a model differentiable with respect to the position and the elements of the map <span class="math inline">M</span>, the <a href="https://en.wikipedia.org/wiki/Multivariate_interpolation">interpolation function</a> is defined to be in the form:</p>
<p><span class="math display">\displaystyle 
  \psi(x) = \sum_{k=1}^K w_k \phi(x, c_k)
</span></p>
<p>where <span class="math inline">x \in \mathbb{R}^2</span> is a point in space, <span class="math inline">w_k</span> the value of the map at the anchor <span class="math inline">k</span> located in position <span class="math inline">c_k</span>, and <span class="math inline">\phi \in (\mathbb{R}^2, \mathbb{R}^2) \mapsto \mathbb{R}</span> is a differentiable radial basis function: it gives a weight to the anchors depending on their distance to the point <span class="math inline">x</span>. In essence, this approach is similar to the attention mechanism in deep learning.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p>With this definition of the scalar potential function <span class="math inline">\psi</span>, the estimated magnetic field <span class="math inline">B</span> is:</p>
<p><span class="math display">\displaystyle 
  B(x) = \nabla_x \psi(x) = \sum_{k=1}^K w_k \nabla_x \phi(x, c_k)
</span></p>
<p><span style="font-size: smaller"> We dropped the negative sign to simplify the equations even though this not standard among mathematicians and physicists, however the true value of the potential is of little interest to our application. </span></p>
<h3 id="radial-basis-functions">Radial basis functions</h3>
<p>A radial basis function satisfies the property <span class="math inline">\phi(x) = \phi(||x||)</span>: its value depends only on the distance from the origin – or another point called the center. Some commonly used RBF are shown below.</p>
<figure>
<img src="/media/magnetic-field-interpolation/rbf.svg" width="350" />
</figure>
<h2 id="learning-the-map-parameters">Learning the map parameters</h2>
<p>Given a set of known points <span class="math inline">S^\star = \{ (x_i^\star, y_i^\star) | i \in [1..N] \}</span> where <span class="math inline">x_i^\star \in \mathbb{R}^2</span> is a position in space and <span class="math inline">y_i^\star \in \mathbb{R}^2</span> the observed value of the <span class="math inline">B</span> field at this position, we are looking for a differentiable function <span class="math inline">\psi \in \mathbb{R}^2 \mapsto \mathbb{R}</span> that minimizes the loss</p>
<p><span class="math display">\displaystyle 
  \mathcal{L}_\psi = \sum_{i=1}^N \delta(\nabla \psi(x_i^\star), y_i^\star)
</span></p>
<p>where <span class="math inline">\delta \in (\mathbb{R}^2, \mathbb{R}^2) \mapsto \mathbb{R}</span> is a measurement of the error between the output of the model and the expected value.</p>
<figure>
<img src="/media/magnetic-field-interpolation/magnetic-field-data.svg" alt="A test case recorded using Optitrack cameras for pose estimation and a calibrated magnetometer. At the center of the scene lies a Sonos wireless speaker which induce a distortion of the magnetic field around it." width="400" /><figcaption>A test case recorded using Optitrack cameras for pose estimation and a calibrated magnetometer. At the center of the scene lies a Sonos wireless speaker which induce a distortion of the magnetic field around it.</figcaption>
</figure>
<h3 id="stochastic-gradient-descent">Stochastic gradient descent</h3>
<p>Considering the parameters as <span class="math inline">\theta = (w_1, \cdots, w_K, c_1, \cdots, c_K)^\top</span>. Minimizing the loss <span class="math inline">\mathcal{L}_\psi</span> using a stochastic gradient descent (SGD) comes down to updating the parameters iteratively using</p>
<p><span class="math display">\displaystyle 
\theta \leftarrow \theta - \epsilon \nabla_\theta \delta(\nabla_x \psi(x^\star), 
y^\star)
</span></p>
<p>where <span class="math inline">\epsilon</span> is a constant controlling the learning rate and <span class="math inline">(x^\star, y^\star)</span> is an element of <span class="math inline">S^\star</span> chosen at random. This is the simplest form of SGD, in practice there is <a href="http://sebastianruder.com/optimizing-gradient-descent/">many improvements</a> to make it much more effective.</p>
<h2 id="implementation-using-a-kalman-filter">Implementation using a Kalman filter</h2>
<div style="font-size: smaller; font-style: italic">
<p>Updated on June 27, 2017.</p>
</div>
<p>We chose <span class="math inline">\phi</span> to be the form of a Gaussian function such that</p>
<p><span class="math display">\displaystyle 
  \phi(x, c_k) = e^{-\frac{||x - c_k||^2}{2 \sigma^2}}
</span></p>
<p>where <span class="math inline">\sigma</span> is a hyperparameter related to the <a href="https://en.wikipedia.org/wiki/Full_width_at_half_maximum">full width at half maximum</a> – it gives the <em>spread</em> of the weighting. Its gradient w.r.t the position <span class="math inline">x</span> is</p>
<p><span class="math display">\displaystyle 
  \nabla_x \phi(x, c_k) = \frac{c_k - x}{\sigma^2}
                          e^{-\frac{||x - c_k||^2}{2 \sigma^2}}
</span></p>
<p>Therefore, we have</p>
<p><span class="math display">\displaystyle 
B(x) = \sum_{k=1}^K w_k \nabla_x \phi(x, c_k)
     = \sum_{k=1}^K w_k \frac{c_k - x}{\sigma^2} e^{-\frac{||x - c_k||^2}{2 \sigma^2}}
</span></p>
<p>By fixing the values of the <span class="math inline">c_k</span> – i.e. taking them as hyperparameters of the model – we can now express <span class="math inline">B(x)</span> as a linear function of the parameters we are trying to estimate (the weights <span class="math inline">w_k</span>) such that:</p>
<p><span class="math display">\displaystyle 
  B = H \cdot w
</span></p>
<p>where <span class="math inline">H</span> is the <span class="math inline">3 \times K</span> matrix</p>
<p><span class="math display">\displaystyle 
H =
\begin{bmatrix}
  \nabla_x \phi(x, c_1) &amp; \cdots &amp; \nabla_x \phi(x, c_K)
\end{bmatrix}
</span></p>
<p>and <span class="math inline">w</span> the column vector</p>
<p><span class="math display">\displaystyle 
w =
\begin{bmatrix}
  w_1 \\
  \vdots \\
  w_K
\end{bmatrix}
</span></p>
<p>When defined as a linear optimization problem, the quest for the most likely field potential fits in the (original) <a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman filter</a> framework – which is, under some conditions, a well suited algorithm for real time estimation of unknown variables. In particular, we are making use of the <em>update</em> step of the algorithm during which observations are used to correct the estimation of the state of the system.</p>
<p>An update iteration of the estimated state <span class="math inline">w</span> and its covariance <span class="math inline">P</span> given a measurement <span class="math inline">y</span> goes as follow:</p>
<div class="algorithm">
<ol type="1">
<li>Compute the <span class="math inline">H</span> matrix for the current position <span class="math inline">x</span></li>
<li>Compute the residual <span class="math inline">z</span> between the true and estimated measurements, its covariance <span class="math inline">S</span> and the Kalman gain <span class="math inline">K</span> using</li>
</ol>
<p><span class="math display">\displaystyle 
\begin{aligned}
z &amp;= y - H w \\
S &amp;= R + H P H^\top \\
K &amp;= P H^\top S^{-1} \\
\end{aligned}
</span></p>
<ol start="3" type="1">
<li>Update the state <span class="math inline">w</span> and its covariance <span class="math inline">P</span> with</li>
</ol>
<p><span class="math display">\displaystyle 
\begin{aligned}
w &amp;\leftarrow w + K z \\
P &amp;\leftarrow (I - K H) P
\end{aligned}
</span></p>
</div>
<p>In our case, the measurement <span class="math inline">y</span> is the output of the magnetometer in <span class="math inline">\mathbb{R}^3</span> (transformed to take into account the orientation of the device). Thus, <span class="math inline">S</span> is in <span class="math inline">\mathcal{M}^{3\times3}</span> and its inverse is easy to compute.</p>
<h3 id="initial-results">Initial results</h3>
<p>In the following experiment, we choose the <span class="math inline">c_k</span> such that the points are spread over a grid covering a least the area of interest with a resolution of 25cm. The model observes 60 samples taken at random from a ground truth made of 380 samples. In other words, the train set is made of <span class="math inline">N = 60</span> samples.</p>
<figure>
<img src="/media/magnetic-field-interpolation/kalman-map-test-1.svg" alt="Model output after one observation of each training sample." id="kalman-map-test" width="400" /><figcaption>Model output after one observation of each training sample.</figcaption>
</figure>
<script type = "text/javascript">
  var images = [], x = 0;
  images[0] = "/media/magnetic-field-interpolation/kalman-map-test-1.svg";
  images[1] = "/media/magnetic-field-interpolation/kalman-map-test-2.svg";

  setInterval(function() {
    x = (x + 1) % images.length;
    document.getElementById("kalman-map-test").src = images[x];
  }, 2000);
</script>
<p>Although the training data do not contain much information on the anomaly at the center of the scene, the model manages to estimate it quite accurately. Indeed, in the figure below we see that the model converges rapidly to its optimum after having seen only a few samples.</p>
<figure>
<img src="/media/magnetic-field-interpolation/kalman-map-mse.svg" alt="Model error evolution during training." width="400" /><figcaption>Model error evolution during training.</figcaption>
</figure>
<h2 id="future-work">Future work</h2>
<h3 id="regularization-using-the-divergence">Regularization using the divergence</h3>
<p>We used only one property of the magnetic field, the divergence-free property could be introduced in the system as a regularizer such that:</p>
<p><span class="math display">\displaystyle 
\nabla \cdot B(x) = \nabla \cdot \nabla \psi(x)
= \sum_{k=1}^K w_k \nabla_x \cdot \nabla_x \phi(x, c_k)
</span></p>
<p>which can be rewritten using the scalar Laplacian:</p>
<p><span class="math display">\displaystyle 
\nabla \cdot B(x) = \sum_{k=1}^K w_k \Delta_x \phi(x, c_k)
</span></p>
<p>In a Kalman filter framework, this would come down to making an observation of the value of the divergence.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Niklas Wahlström et al., “Modeling magnetic fields using Gaussian Processes”, <em>2013 International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Arno Solin et al., “Modeling and interpolation of the ambient magnetic field by Gaussian processes”, <em>arXiv:1509.04634</em><a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Olah &amp; Carter, “Attention and Augmented Recurrent Neural Networks”, <em>Distill, 2016</em>. http://distill.pub/2016/augmented-rnns/<a href="#fnref3">↩</a></p></li>
</ol>
</section>
  </section>
</article>

<script src="/scripts/katex.min.js" />
<script>
  window.onload = function() {
      var elements = document.getElementsByClassName("math");
      Array.prototype.forEach.call(elements, function(e) {
          var str = e.innerHTML.replace(/&amp;/g, "&")
          katex.render(str, e);
      });
  };
</script>
]]></summary>
</entry>
<entry>
    <title>N-dimensional arrays in C</title>
    <link href="http://notsimon.github.io/articles/2012-04-09-c-ndarrays.html" />
    <id>http://notsimon.github.io/articles/2012-04-09-c-ndarrays.html</id>
    <published>2012-04-09T00:00:00Z</published>
    <updated>2012-04-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<style>
  .profile {
    display: none;
  }

  article {
    margin-top: 10vh;
  }
</style>

<article>
  <header>
    <h1>N-dimensional arrays in C</h1>
    
    <p><time>April  9, 2012</time></p>
    
  </header>

  <section class="content">
    <p>The grammar of C is messy and some constructions can be counter-intuitive for beginners: this is the case of multidimensional arrays (non-programmers would call them tensors).</p>
<h2 id="introduction">Introduction</h2>
<p>Someone new to low-level programming would be tempted to declare arrays with multiple dimensions in the following way:</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="dt">float</span>*** make_3d_float_array(<span class="dt">size_t</span> x, <span class="dt">size_t</span> y, <span class="dt">size_t</span> z) {
    <span class="dt">float</span>*** A = malloc(x*<span class="kw">sizeof</span>(<span class="dt">float</span>**));

    <span class="cf">for</span> (...) {
        A[i] = malloc(... <span class="co">// I think you know the rest...</span></code></pre></div>
<p>Even though the syntax for accessing cells is the same as for static arrays, that’s not the best thing to do: unless you’re planning to replace entire rows or planes more often than you do random accesses, this is inefficient. We need contiguous arrays without having to explicitly compute the indexes.</p>
<p>While learning C and experimenting with metaprogramming, I wrote <a href="2012-04-09-c-ndarrays/ndarray.h">a few helpers</a> using the preprocessor for dynamic multidimensional arrays. If you take a look at it without much knowledge of the C preprocessor, you’ll probably see it as mostly voodoo. Fortunately C99 comes with syntactic sugar that makes this unnecessary. <strong>Read on !</strong></p>
<h2 id="static-arrays">Static arrays</h2>
<p>As a reminder, static arrays means we are putting the size of the array in its type. A static 2D arrays of integers can be declared and used as such:</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="dt">void</span> foo(<span class="dt">int</span> x[<span class="dv">4</span>][<span class="dv">2</span>]) {
    <span class="co">// Do something with x.</span>
}

<span class="dt">int</span> main() {
    <span class="dt">int</span> x[<span class="dv">4</span>][<span class="dv">2</span>];

    <span class="cf">for</span> (<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; <span class="kw">sizeof</span>(x) / <span class="kw">sizeof</span>(x[<span class="dv">0</span>]); i++)
        <span class="cf">for</span> (<span class="dt">int</span> j = <span class="dv">0</span>; j &lt; <span class="kw">sizeof</span>(x) / <span class="kw">sizeof</span>(x[<span class="dv">0</span>][<span class="dv">0</span>]); j++)
            x[i][j] = rand();

    foo(x);
    <span class="co">// Don&#39;t be fooled by the syntax of foo&#39;s argument,</span>
    <span class="co">// there&#39;s no copy of x involved!</span>

    <span class="cf">return</span> <span class="dv">0</span>;
}</code></pre></div>
<p>Although the syntax may be deceiving, <strong>the array is not copied when calling the function</strong>. And there’s no way to ask for an implicit copy.</p>
<p>What about dynamic arrays, for which the size may not be known at compile time ?</p>
<h2 id="proper-dynamic-arrays">Proper dynamic arrays</h2>
<p>In C99, variable length arrays can be declared and allocated as follow:</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="dt">int</span> m = rand() % <span class="dv">10</span> + <span class="dv">1</span>;
<span class="dt">int</span> n = rand() % <span class="dv">10</span> + <span class="dv">1</span>;

printf(<span class="st">&quot;m = %d, n = %d</span><span class="sc">\n</span><span class="st">&quot;</span>, m, n);

<span class="dt">int</span> x[m][n];
<span class="dt">int</span> (*y)[n] = malloc(m * n * <span class="kw">sizeof</span>(y[<span class="dv">0</span>][<span class="dv">0</span>]));
<span class="dt">int</span> (*z)[m][n] = malloc(<span class="kw">sizeof</span>(*z));</code></pre></div>
<p><code>x</code> is a dynamic (i.e. variable-length) array allocated on the stack. <code>y</code> and <code>z</code> are both pointers to chunks of memory allocated in the heap (which are the size of <code>x</code>) but the semantic slightly differs between the two: the type of the data <code>y</code> points to is <em>an array of <code>n</code> integers</em>, while <code>z</code> points to <em>an array of <code>m</code> arrays of <code>n</code> integers</em>.</p>
<p>If you’re not comfortable with pointer arithmetic, you may want to read <a href="http://www.cs.umd.edu/class/sum2003/cmsc311/Notes/BitOp/pointer.html">this tutorial</a>.</p>
<p>Cell accesses are written as such:</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c">x[i][j] = <span class="dv">42</span>;
y[i][j] = <span class="dv">42</span>;
(*z)[i][j] = <span class="dv">42</span>; <span class="co">// same as z[0][i][j] ;)</span></code></pre></div>
<p>Even though the semantic of <code>z</code> is, in some way, closer to what we’re expressing, its usage is less practical.</p>
<p>How can we write functions taking dynamic arrays as arguments ? This is were the real syntax tricks hide: we can put the lengths as arguments and use them in the type of the array. For more details on it, read this <a href="https://gcc.gnu.org/onlinedocs/gcc/Variable-Length.html">GCC man page</a>.</p>
<p>In plain C99:</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="dt">void</span> foo(<span class="dt">int</span> m, <span class="dt">int</span> n, <span class="dt">int</span> x[m][n]) {
    <span class="co">// Do something with x.</span>
}

<span class="co">// ...</span>

foo(m, n, x);
foo(m, n, y);
foo(m, n, *z);</code></pre></div>
<p>Again, the arrays are not copied.</p>
<p>Finally, GCC provides an elegant extension to put the size arguments after the array, which may lead to more readable code in some cases:</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="dt">void</span> bar(<span class="dt">int</span> m, <span class="dt">int</span> n; <span class="dt">int</span> x[m][n], <span class="dt">int</span> m, <span class="dt">int</span> n) {
    <span class="co">// Do something with x.</span>
}

<span class="co">// ...</span>

bar(x, m, n);</code></pre></div>
<p>A full working example of this is available <a href="2012-04-09-c-ndarrays/arrays.c">here</a>.</p>
  </section>
</article>

<script src="/scripts/katex.min.js" />
<script>
  window.onload = function() {
      var elements = document.getElementsByClassName("math");
      Array.prototype.forEach.call(elements, function(e) {
          var str = e.innerHTML.replace(/&amp;/g, "&")
          katex.render(str, e);
      });
  };
</script>
]]></summary>
</entry>

</feed>
